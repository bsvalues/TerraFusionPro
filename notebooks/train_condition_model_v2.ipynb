{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TerraFusion Property Condition Model Training v2\n",
    "\n",
    "This notebook demonstrates how to train an improved version of the property condition model using feedback data from users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Feedback Data\n",
    "\n",
    "Load the feedback data from the CSV file and analyze differences between AI scores and user scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the feedback CSV file\n",
    "FEEDBACK_CSV = '../data/condition_feedback.csv'\n",
    "\n",
    "# Load feedback data if it exists\n",
    "if os.path.exists(FEEDBACK_CSV):\n",
    "    df_feedback = pd.read_csv(FEEDBACK_CSV)\n",
    "    print(f\"Loaded {len(df_feedback)} feedback records\")\n",
    "    \n",
    "    # Calculate agreement statistics\n",
    "    df_feedback['agreement'] = df_feedback['ai_score'] == df_feedback['user_score']\n",
    "    df_feedback['difference'] = abs(df_feedback['ai_score'] - df_feedback['user_score'])\n",
    "    \n",
    "    agreement_rate = df_feedback['agreement'].mean()\n",
    "    avg_difference = df_feedback['difference'].mean()\n",
    "    \n",
    "    print(f\"Agreement rate: {agreement_rate:.2%}\")\n",
    "    print(f\"Average score difference: {avg_difference:.2f}\")\n",
    "    \n",
    "    # Identify misclassified examples (where AI and user scores differ)\n",
    "    misclassified = df_feedback[df_feedback['ai_score'] != df_feedback['user_score']]\n",
    "    print(f\"Found {len(misclassified)} misclassified examples\")\n",
    "    \n",
    "    # Show distribution of differences\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(df_feedback['difference'], bins=10)\n",
    "    plt.title('Distribution of Score Differences')\n",
    "    plt.xlabel('Absolute Difference')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Feedback file not found at {FEEDBACK_CSV}\")\n",
    "    print(\"Run the application and collect user feedback first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Dataset with Feedback-Enhanced Labeling\n",
    "\n",
    "Create a custom dataset that includes feedback corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropertyConditionDataset(Dataset):\n",
    "    \"\"\"Dataset for property condition images with user feedback integration\"\"\"\n",
    "    \n",
    "    def __init__(self, feedback_df, uploads_dir='../uploads', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            feedback_df: DataFrame with feedback data\n",
    "            uploads_dir: Directory containing the uploaded images\n",
    "            transform: Optional transform to be applied to images\n",
    "        \"\"\"\n",
    "        self.feedback_df = feedback_df\n",
    "        self.uploads_dir = uploads_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Filter to keep only entries with existing image files\n",
    "        valid_entries = []\n",
    "        for idx, row in feedback_df.iterrows():\n",
    "            img_path = os.path.join(uploads_dir, row['filename'])\n",
    "            if os.path.exists(img_path):\n",
    "                valid_entries.append(idx)\n",
    "                \n",
    "        self.feedback_df = feedback_df.iloc[valid_entries].reset_index(drop=True)\n",
    "        print(f\"Dataset contains {len(self.feedback_df)} valid entries with images\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.feedback_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get image path\n",
    "        img_path = os.path.join(self.uploads_dir, self.feedback_df.iloc[idx]['filename'])\n",
    "        \n",
    "        # Load and convert image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Apply transformations if specified\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Get user score as ground truth (preferred over AI score)\n",
    "        # Normalize to range 0-4 for classification (from 1-5)\n",
    "        score = self.feedback_df.iloc[idx]['user_score'] - 1\n",
    "        \n",
    "        # Return image and target score\n",
    "        return image, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Model Architecture\n",
    "\n",
    "Use MobileNetV2 with pretrained weights as the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # Load pre-trained MobileNetV2 model\n",
    "    model = models.mobilenet_v2(pretrained=True)\n",
    "    \n",
    "    # Modify the classifier to output 5 classes (condition levels 1-5)\n",
    "    num_ftrs = model.classifier[1].in_features\n",
    "    model.classifier[1] = nn.Linear(num_ftrs, 5)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop with Misclassification Prioritization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=10, device='cpu'):\n",
    "    # Initialize tracking variables\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    \n",
    "    # Start training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # Iterate over data batches\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels.long())\n",
    "                    \n",
    "                    # Backward + optimize only in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "            # Save history\n",
    "            if phase == 'train':\n",
    "                train_loss_history.append(epoch_loss)\n",
    "                train_acc_history.append(epoch_acc.item())\n",
    "            else:\n",
    "                val_loss_history.append(epoch_loss)\n",
    "                val_acc_history.append(epoch_acc.item())\n",
    "            \n",
    "            # Save best model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = model.state_dict().copy()\n",
    "                \n",
    "        print()\n",
    "    \n",
    "    # Print training results\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    # Return model and history\n",
    "    return model, {\n",
    "        'train_loss': train_loss_history,\n",
    "        'val_loss': val_loss_history,\n",
    "        'train_acc': train_acc_history,\n",
    "        'val_acc': val_acc_history\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Main Training Workflow with Misclassified Prioritization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if feedback data exists and proceed with training\n",
    "if os.path.exists(FEEDBACK_CSV) and len(df_feedback) > 10:  # Ensure enough samples\n",
    "    # Define transformations\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    \n",
    "    # Priority sampling for misclassified examples\n",
    "    # Oversample misclassified examples to focus training on them\n",
    "    if len(misclassified) > 0:\n",
    "        print(\"Applying priority sampling for misclassified examples...\")\n",
    "        # Duplicate misclassified samples to increase their weight\n",
    "        augmented_df = pd.concat([df_feedback, misclassified, misclassified])\n",
    "        print(f\"Augmented dataset size: {len(augmented_df)} entries\")\n",
    "    else:\n",
    "        augmented_df = df_feedback\n",
    "    \n",
    "    # Split data into train and validation sets\n",
    "    train_df, val_df = train_test_split(augmented_df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = PropertyConditionDataset(train_df, transform=data_transforms['train'])\n",
    "    val_dataset = PropertyConditionDataset(val_df, transform=data_transforms['val'])\n",
    "    \n",
    "    # Create dataloaders\n",
    "    dataloaders = {\n",
    "        'train': DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4),\n",
    "        'val': DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)\n",
    "    }\n",
    "    \n",
    "    # Set device for training\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create the model\n",
    "    model = create_model()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Try to load existing model weights for fine-tuning\n",
    "    MODEL_PATH = '../models/condition_model.pth'\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        print(f\"Loading existing model from {MODEL_PATH} for fine-tuning\")\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "    \n",
    "    # Set up loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"\\nStarting model training...\")\n",
    "    trained_model, history = train_model(model, dataloaders, criterion, optimizer,\n",
    "                                         num_epochs=15, device=device)\n",
    "    \n",
    "    # Save the model\n",
    "    MODEL_DIR = '../models'\n",
    "    if not os.path.exists(MODEL_DIR):\n",
    "        os.makedirs(MODEL_DIR)\n",
    "    \n",
    "    # Save with version number and timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_v2_path = f\"{MODEL_DIR}/condition_model_v2_{timestamp}.pth\"\n",
    "    torch.save(trained_model.state_dict(), model_v2_path)\n",
    "    \n",
    "    # Also save as the default model\n",
    "    torch.save(trained_model.state_dict(), MODEL_PATH)\n",
    "    \n",
    "    print(f\"\\nModel saved to {model_v2_path}\")\n",
    "    print(f\"Updated default model at {MODEL_PATH}\")\n",
    "    \n",
    "    # Plot training results\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train')\n",
    "    plt.plot(history['val_loss'], label='Validation')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train')\n",
    "    plt.plot(history['val_acc'], label='Validation')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Log to model training history\n",
    "    TRAINING_LOG = f\"{MODEL_DIR}/training_history.csv\"\n",
    "    log_exists = os.path.exists(TRAINING_LOG)\n",
    "    \n",
    "    with open(TRAINING_LOG, 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not log_exists:\n",
    "            writer.writerow(['timestamp', 'model_version', 'training_samples', 'misclassified_samples', 'val_accuracy'])\n",
    "        \n",
    "        writer.writerow([\n",
    "            timestamp,\n",
    "            'v2',\n",
    "            len(train_dataset),\n",
    "            len(misclassified),\n",
    "            best_acc.item()\n",
    "        ])\n",
    "    \n",
    "    print(f\"Training log updated at {TRAINING_LOG}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Insufficient feedback data for training.\")\n",
    "    print(\"Please collect more user feedback before training the v2 model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model performance on feedback data\n",
    "def evaluate_model_on_feedback(model, feedback_df, uploads_dir='../uploads', device='cpu'):\n",
    "    model.eval()\n",
    "    \n",
    "    # Define transform for evaluation\n",
    "    eval_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, row in feedback_df.iterrows():\n",
    "            img_path = os.path.join(uploads_dir, row['filename'])\n",
    "            if not os.path.exists(img_path):\n",
    "                continue\n",
    "                \n",
    "            # Load and transform image\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            image_tensor = eval_transform(image).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Get model prediction\n",
    "            outputs = model(image_tensor)\n",
    "            _, pred_class = torch.max(outputs, 1)\n",
    "            predicted_score = pred_class.item() + 1  # Convert back to 1-5 scale\n",
    "            \n",
    "            # Get softmax probabilities\n",
    "            probabilities = torch.nn.functional.softmax(outputs, dim=1)[0]\n",
    "            \n",
    "            # Calculate weighted average score\n",
    "            weighted_score = 0\n",
    "            for i in range(5):\n",
    "                weighted_score += (i + 1) * probabilities[i].item()\n",
    "                \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'filename': row['filename'],\n",
    "                'ai_score_old': row['ai_score'],\n",
    "                'user_score': row['user_score'],\n",
    "                'predicted_score': predicted_score,\n",
    "                'weighted_score': weighted_score\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    if len(results_df) > 0:\n",
    "        # Calculate improvement metrics\n",
    "        results_df['old_error'] = abs(results_df['ai_score_old'] - results_df['user_score'])\n",
    "        results_df['new_error'] = abs(results_df['weighted_score'] - results_df['user_score'])\n",
    "        results_df['improvement'] = results_df['old_error'] - results_df['new_error']\n",
    "        \n",
    "        avg_old_error = results_df['old_error'].mean()\n",
    "        avg_new_error = results_df['new_error'].mean()\n",
    "        avg_improvement = results_df['improvement'].mean()\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nModel Evaluation Results:\")\n",
    "        print(f\"Average error with old model: {avg_old_error:.4f}\")\n",
    "        print(f\"Average error with new model: {avg_new_error:.4f}\")\n",
    "        print(f\"Average improvement: {avg_improvement:.4f} ({(avg_improvement/avg_old_error)*100:.1f}%)\")\n",
    "        \n",
    "        # Plot error comparison\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.scatter(results_df['user_score'], results_df['ai_score_old'], alpha=0.7, label='Old Model')\n",
    "        plt.scatter(results_df['user_score'], results_df['weighted_score'], alpha=0.7, label='New Model')\n",
    "        plt.plot([1, 5], [1, 5], 'k--')  # Ideal prediction line\n",
    "        plt.xlabel('User Score')\n",
    "        plt.ylabel('Model Score')\n",
    "        plt.title('Score Comparison')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.hist(results_df['old_error'], alpha=0.7, bins=10, label='Old Model Error')\n",
    "        plt.hist(results_df['new_error'], alpha=0.7, bins=10, label='New Model Error')\n",
    "        plt.xlabel('Absolute Error')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Error Distribution')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return results_df\n",
    "    else:\n",
    "        print(\"No valid images found for evaluation\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation if model was trained successfully\n",
    "if 'trained_model' in locals():\n",
    "    print(\"Evaluating trained model on feedback data...\")\n",
    "    evaluation_results = evaluate_model_on_feedback(trained_model, df_feedback, device=device)\n",
    "else:\n",
    "    print(\"Model training was not completed. Cannot evaluate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps\n",
    "\n",
    "- The model has been retrained with user feedback to improve accuracy\n",
    "- Misclassified examples were prioritized in training\n",
    "- Performance metrics show the improvement from v1 to v2\n",
    "- The new model is ready for deployment\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Continue collecting user feedback to further refine the model\n",
    "2. Consider adding more types of image augmentation\n",
    "3. Explore more advanced architectures or ensemble methods\n",
    "4. Implement feature importance visualization (SHAP values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}